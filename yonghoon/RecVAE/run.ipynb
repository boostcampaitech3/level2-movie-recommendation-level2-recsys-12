{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "from utils import get_data, ndcg, recall\n",
    "from model import VAE\n",
    "\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    'dataset': \"output\",\n",
    "    'hidden_dim': 600,\n",
    "    'latent_dim': 200,\n",
    "    'batch_size': 500,\n",
    "    'beta': None,\n",
    "    'gamma': 0.005,\n",
    "    'lr': 5e-4,\n",
    "    'n_epochs': 50,\n",
    "    'n_enc_epochs': 3,\n",
    "    'n_dec_epochs': 3,\n",
    "    'n_dec_epochs': 1,\n",
    "    'not_alternating': False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25d8e95ca50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1337\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(args.dataset)\n",
    "train_data, valid_in_data, valid_out_data, test_in_data, test_out_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, device, idx, data_in, data_out=None):\n",
    "        self._device = device\n",
    "        self._idx = idx\n",
    "        self._data_in = data_in\n",
    "        self._data_out = data_out\n",
    "    \n",
    "    def get_idx(self):\n",
    "        return self._idx\n",
    "    \n",
    "    def get_idx_to_dev(self):\n",
    "        return torch.LongTensor(self.get_idx()).to(self._device)\n",
    "        \n",
    "    def get_ratings(self, is_out=False):\n",
    "        data = self._data_out if is_out else self._data_in\n",
    "        return data[self._idx]\n",
    "    \n",
    "    def get_ratings_to_dev(self, is_out=False):\n",
    "        return torch.Tensor(\n",
    "            self.get_ratings(is_out).toarray()\n",
    "        ).to(self._device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, device, data_in, data_out=None, shuffle=False, samples_perc_per_epoch=1):\n",
    "    assert 0 < samples_perc_per_epoch <= 1\n",
    "    \n",
    "    total_samples = data_in.shape[0]\n",
    "    samples_per_epoch = int(total_samples * samples_perc_per_epoch)\n",
    "    \n",
    "    if shuffle:\n",
    "        idxlist = np.arange(total_samples)\n",
    "        np.random.shuffle(idxlist)\n",
    "        idxlist = idxlist[:samples_per_epoch]\n",
    "    else:\n",
    "        idxlist = np.arange(samples_per_epoch)\n",
    "    \n",
    "    for st_idx in range(0, samples_per_epoch, batch_size):\n",
    "        end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
    "        idx = idxlist[st_idx:end_idx]\n",
    "\n",
    "        yield Batch(device, idx, data_in, data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_in, data_out, metrics, samples_perc_per_epoch=1, batch_size=500):\n",
    "    metrics = deepcopy(metrics)\n",
    "    model.eval()\n",
    "    \n",
    "    for m in metrics:\n",
    "        m['score'] = []\n",
    "    \n",
    "    for batch in generate(batch_size=batch_size,\n",
    "                          device=device,\n",
    "                          data_in=data_in,\n",
    "                          data_out=data_out,\n",
    "                          samples_perc_per_epoch=samples_perc_per_epoch\n",
    "                         ):\n",
    "        \n",
    "        ratings_in = batch.get_ratings_to_dev()\n",
    "        ratings_out = batch.get_ratings(is_out=True)\n",
    "    \n",
    "        ratings_pred = model(ratings_in, calculate_loss=False).cpu().detach().numpy()\n",
    "        \n",
    "        if not (data_in is data_out):\n",
    "            ratings_pred[batch.get_ratings().nonzero()] = -np.inf\n",
    "            \n",
    "        for m in metrics:\n",
    "            m['score'].append(m['metric'](ratings_pred, ratings_out, k=m['k']))\n",
    "\n",
    "    for m in metrics:\n",
    "        m['score'] = np.concatenate(m['score']).mean()\n",
    "        \n",
    "    return [x['score'] for x in metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, opts, train_data, batch_size, n_epochs, beta, gamma, dropout_rate):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in generate(batch_size=batch_size, device=device, data_in=train_data, shuffle=True):\n",
    "            ratings = batch.get_ratings_to_dev()\n",
    "\n",
    "            for optimizer in opts:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            _, loss = model(ratings, beta=beta, gamma=gamma, dropout_rate=dropout_rate)\n",
    "            loss.backward()\n",
    "            \n",
    "            for optimizer in opts:\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    'hidden_dim': args.hidden_dim,\n",
    "    'latent_dim': args.latent_dim,\n",
    "    'input_dim': train_data.shape[1]\n",
    "}\n",
    "metrics = [{'metric': ndcg, 'k': 100}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ndcg = -np.inf\n",
    "train_scores, valid_scores = [], []\n",
    "\n",
    "model = VAE(**model_kwargs).to(device)\n",
    "model_best = VAE(**model_kwargs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_kwargs = {\n",
    "    'model': model,\n",
    "    'train_data': train_data,\n",
    "    'batch_size': args.batch_size,\n",
    "    'beta': args.beta,\n",
    "    'gamma': args.gamma\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_params = set(model.decoder.parameters())\n",
    "encoder_params = set(model.encoder.parameters())\n",
    "\n",
    "optimizer_encoder = optim.Adam(encoder_params, lr=args.lr)\n",
    "optimizer_decoder = optim.Adam(decoder_params, lr=args.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | valid ndcg@100: 0.0781 | best valid: 0.0781 | train ndcg@100: 0.1207\n",
      "epoch 1 | valid ndcg@100: 0.1005 | best valid: 0.1005 | train ndcg@100: 0.1433\n",
      "epoch 2 | valid ndcg@100: 0.1106 | best valid: 0.1106 | train ndcg@100: 0.1544\n",
      "epoch 3 | valid ndcg@100: 0.1186 | best valid: 0.1186 | train ndcg@100: 0.1593\n",
      "epoch 4 | valid ndcg@100: 0.1207 | best valid: 0.1207 | train ndcg@100: 0.1627\n",
      "epoch 5 | valid ndcg@100: 0.1240 | best valid: 0.1240 | train ndcg@100: 0.1650\n",
      "epoch 6 | valid ndcg@100: 0.1259 | best valid: 0.1259 | train ndcg@100: 0.1669\n",
      "epoch 7 | valid ndcg@100: 0.1269 | best valid: 0.1269 | train ndcg@100: 0.1685\n",
      "epoch 8 | valid ndcg@100: 0.1277 | best valid: 0.1277 | train ndcg@100: 0.1699\n",
      "epoch 9 | valid ndcg@100: 0.1288 | best valid: 0.1288 | train ndcg@100: 0.1713\n",
      "epoch 10 | valid ndcg@100: 0.1315 | best valid: 0.1315 | train ndcg@100: 0.1724\n",
      "epoch 11 | valid ndcg@100: 0.1316 | best valid: 0.1316 | train ndcg@100: 0.1735\n",
      "epoch 12 | valid ndcg@100: 0.1313 | best valid: 0.1316 | train ndcg@100: 0.1746\n",
      "epoch 13 | valid ndcg@100: 0.1323 | best valid: 0.1323 | train ndcg@100: 0.1761\n",
      "epoch 14 | valid ndcg@100: 0.1328 | best valid: 0.1328 | train ndcg@100: 0.1768\n",
      "epoch 15 | valid ndcg@100: 0.1331 | best valid: 0.1331 | train ndcg@100: 0.1780\n",
      "epoch 16 | valid ndcg@100: 0.1349 | best valid: 0.1349 | train ndcg@100: 0.1794\n",
      "epoch 17 | valid ndcg@100: 0.1349 | best valid: 0.1349 | train ndcg@100: 0.1796\n",
      "epoch 18 | valid ndcg@100: 0.1348 | best valid: 0.1349 | train ndcg@100: 0.1815\n",
      "epoch 19 | valid ndcg@100: 0.1358 | best valid: 0.1358 | train ndcg@100: 0.1824\n",
      "epoch 20 | valid ndcg@100: 0.1351 | best valid: 0.1358 | train ndcg@100: 0.1827\n",
      "epoch 21 | valid ndcg@100: 0.1347 | best valid: 0.1358 | train ndcg@100: 0.1840\n",
      "epoch 22 | valid ndcg@100: 0.1352 | best valid: 0.1358 | train ndcg@100: 0.1850\n",
      "epoch 23 | valid ndcg@100: 0.1341 | best valid: 0.1358 | train ndcg@100: 0.1857\n",
      "epoch 24 | valid ndcg@100: 0.1354 | best valid: 0.1358 | train ndcg@100: 0.1864\n",
      "epoch 25 | valid ndcg@100: 0.1350 | best valid: 0.1358 | train ndcg@100: 0.1882\n",
      "epoch 26 | valid ndcg@100: 0.1352 | best valid: 0.1358 | train ndcg@100: 0.1884\n",
      "epoch 27 | valid ndcg@100: 0.1361 | best valid: 0.1361 | train ndcg@100: 0.1897\n",
      "epoch 28 | valid ndcg@100: 0.1377 | best valid: 0.1377 | train ndcg@100: 0.1903\n",
      "epoch 29 | valid ndcg@100: 0.1366 | best valid: 0.1377 | train ndcg@100: 0.1898\n",
      "epoch 30 | valid ndcg@100: 0.1368 | best valid: 0.1377 | train ndcg@100: 0.1907\n",
      "epoch 31 | valid ndcg@100: 0.1351 | best valid: 0.1377 | train ndcg@100: 0.1916\n",
      "epoch 32 | valid ndcg@100: 0.1352 | best valid: 0.1377 | train ndcg@100: 0.1929\n",
      "epoch 33 | valid ndcg@100: 0.1362 | best valid: 0.1377 | train ndcg@100: 0.1935\n",
      "epoch 34 | valid ndcg@100: 0.1355 | best valid: 0.1377 | train ndcg@100: 0.1941\n",
      "epoch 35 | valid ndcg@100: 0.1360 | best valid: 0.1377 | train ndcg@100: 0.1953\n",
      "epoch 36 | valid ndcg@100: 0.1364 | best valid: 0.1377 | train ndcg@100: 0.1961\n",
      "epoch 37 | valid ndcg@100: 0.1363 | best valid: 0.1377 | train ndcg@100: 0.1974\n",
      "epoch 38 | valid ndcg@100: 0.1362 | best valid: 0.1377 | train ndcg@100: 0.1983\n",
      "epoch 39 | valid ndcg@100: 0.1361 | best valid: 0.1377 | train ndcg@100: 0.1983\n",
      "epoch 40 | valid ndcg@100: 0.1374 | best valid: 0.1377 | train ndcg@100: 0.1986\n",
      "epoch 41 | valid ndcg@100: 0.1366 | best valid: 0.1377 | train ndcg@100: 0.1994\n",
      "epoch 42 | valid ndcg@100: 0.1367 | best valid: 0.1377 | train ndcg@100: 0.2003\n",
      "epoch 43 | valid ndcg@100: 0.1367 | best valid: 0.1377 | train ndcg@100: 0.2006\n",
      "epoch 44 | valid ndcg@100: 0.1354 | best valid: 0.1377 | train ndcg@100: 0.2020\n",
      "epoch 45 | valid ndcg@100: 0.1349 | best valid: 0.1377 | train ndcg@100: 0.2026\n",
      "epoch 46 | valid ndcg@100: 0.1370 | best valid: 0.1377 | train ndcg@100: 0.2037\n",
      "epoch 47 | valid ndcg@100: 0.1375 | best valid: 0.1377 | train ndcg@100: 0.2034\n",
      "epoch 48 | valid ndcg@100: 0.1362 | best valid: 0.1377 | train ndcg@100: 0.2036\n",
      "epoch 49 | valid ndcg@100: 0.1353 | best valid: 0.1377 | train ndcg@100: 0.2047\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.n_epochs):\n",
    "\n",
    "    if args.not_alternating:\n",
    "        run(opts=[optimizer_encoder, optimizer_decoder], n_epochs=1, dropout_rate=0.5, **learning_kwargs)\n",
    "    else:\n",
    "        run(opts=[optimizer_encoder], n_epochs=args.n_enc_epochs, dropout_rate=0.5, **learning_kwargs)\n",
    "        model.update_prior()\n",
    "        run(opts=[optimizer_decoder], n_epochs=args.n_dec_epochs, dropout_rate=0, **learning_kwargs)\n",
    "\n",
    "    train_scores.append(\n",
    "        evaluate(model, train_data, train_data, metrics, 0.01)[0]\n",
    "    )\n",
    "    valid_scores.append(\n",
    "        evaluate(model, valid_in_data, valid_out_data, metrics, 1)[0]\n",
    "    )\n",
    "    \n",
    "    if valid_scores[-1] > best_ndcg:\n",
    "        best_ndcg = valid_scores[-1]\n",
    "        model_best.load_state_dict(deepcopy(model.state_dict()))\n",
    "        \n",
    "\n",
    "    print(f'epoch {epoch} | valid ndcg@100: {valid_scores[-1]:.4f} | ' +\n",
    "          f'best valid: {best_ndcg:.4f} | train ndcg@100: {train_scores[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@100:\t0.0932\n",
      "recall@20:\t0.0751\n",
      "recall@50:\t0.0854\n"
     ]
    }
   ],
   "source": [
    "test_metrics = [{'metric': ndcg, 'k': 100}, {'metric': recall, 'k': 20}, {'metric': recall, 'k': 50}]\n",
    "\n",
    "final_scores = evaluate(model_best, test_in_data, test_out_data, test_metrics)\n",
    "\n",
    "for metric, score in zip(test_metrics, final_scores):\n",
    "    print(f\"{metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_best, \"RecVAE_50_epoch\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fc799023037d79fb4b044712171da9d153d1f894f3734afcc893660f54fa373"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
