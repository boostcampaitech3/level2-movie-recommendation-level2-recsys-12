{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "from box import Box\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setSeed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Preprocessing\n",
    "    \n",
    "* Implicit Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataInfo():\n",
    "    def __init__(self, file_path: str, num_valid: int) -> None:\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.user_encoder, self.movie_encoder = self._encode()\n",
    "\n",
    "        self.users = self.df['user'].unique()\n",
    "        self.num_users = len(self.users)\n",
    "        self.min_feedback = 5 # for resolving cold-start problem\n",
    "        self.movies = self.df['item'].unique()\n",
    "        self.num_movies = len(self.movies)\n",
    "        self.train_movies = self._moreThanFeedback()\n",
    "        self.num_train_movies = len(self.train_movies)\n",
    "\n",
    "        self.df_pos_user_sequence, self.user_negative_samples = self._makeSequenceAndNeg()\n",
    "\n",
    "        self.num_valid = num_valid\n",
    "        self.df_pos_train, self.df_pos_test = self._trainTestSplit()\n",
    "    \n",
    "    def _trainTestSplit(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        # https://github.com/scikit-learn/scikit-learn/pull/16236\n",
    "        # GroupTimeSeriesSplit이 PR 중이라고 해서 직접 구현을 해야한다.\n",
    "\n",
    "        print('train and test split...')\n",
    "        df_train = {\n",
    "            'user_id': list(),\n",
    "            'movie_id': list(),\n",
    "        }\n",
    "\n",
    "        df_test = {\n",
    "            'user_id': list(),\n",
    "            'movie_id': list(),\n",
    "        }\n",
    "\n",
    "        for user in tqdm(self.users):\n",
    "            df_user = self.df_pos_user_sequence[self.df_pos_user_sequence['user_id']==user]\n",
    "            list_user_movies = df_user['movie_id'].tolist()\n",
    "\n",
    "            num_user = df_user.shape[0]\n",
    "            num_train_user = num_user - self.num_valid\n",
    "            num_test_user = self.num_valid\n",
    "\n",
    "            list_user_train_movies = list_user_movies[:num_train_user]\n",
    "            list_user_test_movies = list_user_movies[num_train_user:]\n",
    "\n",
    "            df_train['user_id'].extend([user]*num_train_user)\n",
    "            df_train['movie_id'].extend(list_user_train_movies)\n",
    "\n",
    "            df_test['user_id'].extend([user]*num_test_user)\n",
    "            df_test['movie_id'].extend(list_user_test_movies)\n",
    "            \n",
    "        df_train = pd.DataFrame(df_train)\n",
    "        df_train['label'] = 1\n",
    "\n",
    "        df_test = pd.DataFrame(df_test)\n",
    "        df_test['label'] = 1\n",
    "\n",
    "        print('done!')\n",
    "            \n",
    "        return df_train, df_test        \n",
    "\n",
    "\n",
    "    def _moreThanFeedback(self):\n",
    "        print('pick trainable feedback...')\n",
    "        \n",
    "        movie_ids_for_training = list()\n",
    "        for movie_id in tqdm(self.df['item'].unique()):\n",
    "            if self.df[self.df['item'] == movie_id].shape[0] >= self.min_feedback:\n",
    "                movie_ids_for_training.append(movie_id)\n",
    "        \n",
    "        print('doen!')\n",
    "        \n",
    "        return np.unique(movie_ids_for_training)\n",
    "\n",
    "\n",
    "    def _makeSequenceAndNeg(self):\n",
    "\n",
    "        print('sorted by sequence and make negative smaples...')\n",
    "        pos_user_sequence = {\n",
    "            'user_id': list(),\n",
    "            'movie_id': list(),\n",
    "        }\n",
    "\n",
    "        user_negative_samples = dict()\n",
    "\n",
    "\n",
    "        for user in tqdm(self.users):\n",
    "            user_sequence_movies = self.df[self.df['user']==user].sort_values(by='time', axis=0)['item'].tolist()\n",
    "            # 최소 조건을 만족하지 못한 것 포함, feedback있는 영화를 제외시킨다. \n",
    "            user_negative_movies = np.setdiff1d(self.movies, np.unique(user_sequence_movies))\n",
    "            user_negative_samples[user] = user_negative_movies\n",
    "\n",
    "            for movie in user_sequence_movies:\n",
    "                # 최소 feedback 조건을 만족시키지 못한 것은 추가하지 않는다.\n",
    "                if movie not in self.train_movies: continue\n",
    "                pos_user_sequence['user_id'].append(user)\n",
    "                pos_user_sequence['movie_id'].append(movie)\n",
    "\n",
    "\n",
    "        df_pos_user_sequence = pd.DataFrame(pos_user_sequence)\n",
    "        df_pos_user_sequence['label'] = 1\n",
    "\n",
    "        print('doen!')\n",
    "\n",
    "        return df_pos_user_sequence, user_negative_samples\n",
    "\n",
    "\n",
    "    def _encode(self) -> Tuple[pd.DataFrame, LabelEncoder, LabelEncoder]:\n",
    "        print('encoding...')\n",
    "\n",
    "        userId_label_encoder = LabelEncoder()\n",
    "        movieId_label_encoder = LabelEncoder()\n",
    "\n",
    "        self.df['user'] = userId_label_encoder.fit_transform(self.df['user'].values)\n",
    "        self.df['item'] = movieId_label_encoder.fit_transform(self.df['item'].values)\n",
    "\n",
    "        print('done!')\n",
    "\n",
    "        # encoder.inverse_transform() 으로 decode\n",
    "        return userId_label_encoder, movieId_label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.df.iloc[idx]['user_id']\n",
    "        item = self.df.iloc[idx]['movie_id']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        \n",
    "        return torch.tensor(user).to(torch.int64), torch.tensor(item).to(torch.int64), torch.tensor(label).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, num_user):\n",
    "        self.df = df\n",
    "        self.users = [i for i in range(num_user)]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        \n",
    "        items = self.df[self.df['user_id']==user]['movie_id'].to_numpy()\n",
    "        users = [user] * len(items)\n",
    "        labels = [1] * len(items)\n",
    "        \n",
    "        return torch.tensor(users).to(torch.int64) , torch.tensor(items).to(torch.int64), torch.tensor(labels).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users: int, num_items: int, latent_dim: int):\n",
    "        super(GMF, self).__init__()\n",
    "\n",
    "        self.embedding_user = nn.Embedding(num_users, latent_dim)\n",
    "        self.embedding_item = nn.Embedding(num_items, latent_dim)\n",
    "\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1, bias=False),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            # print(module.__class__.__name__)\n",
    "            if isinstance(module, nn.Embedding):\n",
    "                nn.init.xavier_normal_(module.weight.data)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight.data)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.fill_(0.0)\n",
    "\n",
    "    \n",
    "    def forward(self, user_input, item_input):\n",
    "\n",
    "        user_latent = self.embedding_user(user_input)\n",
    "        item_latent = self.embedding_item(item_input)\n",
    "        product = user_latent * item_latent\n",
    "\n",
    "        output = self.prediction(product)\n",
    "\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_uesrs, num_items, latent_dim, dropout, layers=[20, 10]):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.embedding_user = nn.Embedding(num_uesrs, latent_dim)\n",
    "        self.embedding_item = nn.Embedding(num_items, latent_dim)\n",
    "\n",
    "        self.layers = deepcopy(layers)\n",
    "        self.layers.insert(0, latent_dim * 2)\n",
    "        modules = []\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            modules.append(nn.Dropout(p=dropout))\n",
    "            modules.append(nn.Linear(self.layers[i], self.layers[i+1]))\n",
    "            modules.append(nn.ReLU())\n",
    "        \n",
    "        self.dense_layers = nn.Sequential(*modules)\n",
    "\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Linear(layers[-1], 1, bias=False),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "    \n",
    "    # initialize weights\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Embedding):\n",
    "                nn.init.xavier_normal_(module.weight.data)\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight.data)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        user_latent = self.embedding_user(user_input)\n",
    "        item_latent = self.embedding_item(item_input)\n",
    "\n",
    "        vector = torch.cat((user_latent, item_latent), dim=-1)\n",
    "\n",
    "        output = self.prediction(self.dense_layers(vector))\n",
    "\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, GMF, MLP, latent_dim, layers):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        self.GMF_embedding_user = deepcopy(GMF.embedding_user)\n",
    "        self.GMF_embedding_item = deepcopy(GMF.embedding_item)\n",
    "\n",
    "        self.MLP_embedding_user = deepcopy(MLP.embedding_user)\n",
    "        self.MLP_embedding_item = deepcopy(MLP.embedding_item)\n",
    "\n",
    "        self.MLP_dense_layers = deepcopy(MLP.dense_layers)\n",
    "\n",
    "        self.NeuMF_layer = nn.Sequential(\n",
    "            nn.Linear(latent_dim + layers[-1], 1, bias=False),\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    # initialize weights\n",
    "    def _init_weights(self):\n",
    "        for module in self.NeuMF_layer.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight.data)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "\n",
    "        GMF_user_latent = self.GMF_embedding_user(user_input)\n",
    "        GMF_item_latent = self.GMF_embedding_item(item_input)\n",
    "        GMF_output = GMF_user_latent * GMF_item_latent\n",
    "\n",
    "        MLP_user_latent = self.MLP_embedding_user(user_input)\n",
    "        MLP_item_latent = self.MLP_embedding_item(item_input)\n",
    "        vector = torch.cat((MLP_user_latent, MLP_item_latent), dim=-1)\n",
    "        MLP_output = self.MLP_dense_layers(vector)\n",
    "\n",
    "        concat_output = torch.cat((GMF_output, MLP_output), dim=-1)\n",
    "        output = self.NeuMF_layer(concat_output)\n",
    "\n",
    "        return output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, positive, negative):\n",
    "        distances = positive - negative\n",
    "        loss = -torch.mean(self.logsigmoid(distances))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train, Metric 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, train_loader, test_loader, epochs, criterion, device, neg_smaples, num_neg, topK, output_path, name):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.epochs = epochs\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.neg_samples = neg_smaples\n",
    "        self.num_neg = num_neg\n",
    "        self.topK = topK\n",
    "\n",
    "        self.loss_list = []\n",
    "        self.recall_list = []\n",
    "        self.ndcg_list = []\n",
    "\n",
    "        self.output_path = output_path\n",
    "        self.model_name = name\n",
    "        \n",
    "        self._train_metric()\n",
    "    \n",
    "\n",
    "    def _train_metric(self):\n",
    "        \n",
    "        best_ndcg = 0\n",
    "        # 훈련 시간 측정\n",
    "        epoch_start = torch.cuda.Event(enable_timing=True)\n",
    "        epoch_end = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            # 시작 시간 기록\n",
    "            epoch_start.record()\n",
    "\n",
    "            avg_loss = self._train()\n",
    "            avg_recall, avg_ndcg = self._metric()\n",
    "        \n",
    "            epoch_end.record()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            self.loss_list.append(avg_loss)\n",
    "            self.recall_list.append(avg_recall)\n",
    "            self.ndcg_list.append(avg_ndcg)\n",
    "\n",
    "\n",
    "            print(\n",
    "                f'Epoch[{epoch+1}/{self.epochs}]\\ttrain_loss: {avg_loss:.4f}' +\n",
    "                f'\\trecall: {avg_recall:.4f}\\tNDCG: {avg_ndcg:.4f} '+\n",
    "                f'\\t훈련시간: {epoch_start.elapsed_time(epoch_end)/1000:.2f} sec'\n",
    "            )\n",
    "\n",
    "            if best_ndcg < avg_ndcg:\n",
    "                best_ndcg = avg_ndcg\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.output_path, f'best_NDCG_{self.model_name}.pt'))\n",
    "                print(f'save ndcg: {best_ndcg:.4f}')\n",
    "\n",
    "    \n",
    "    def _train(self):\n",
    "        self.model.train()\n",
    "\n",
    "        size = len(self.train_loader)\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for user, item, label in tqdm(self.train_loader):\n",
    "            user = user.to(self.device)\n",
    "            item = item.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "\n",
    "            user_neg, item_neg, label_neg = self._negative_sampling(user, self.num_neg)\n",
    "            user_pos, item_pos, label_pos = self._multiple_users(user, item, self.num_neg)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output_pos = self.model(user_pos, item_pos)\n",
    "            output_neg = self.model(user_neg, item_neg)\n",
    "\n",
    "            loss = self.criterion(output_pos, output_neg)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "        avg_loss = epoch_loss / size\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def _metric(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        size = len(self.test_loader)\n",
    "\n",
    "        epoch_recall = 0\n",
    "        epoch_ndcg = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for user, item, label in self.test_loader:\n",
    "                user = user.to(self.device).squeeze(dim=0)\n",
    "                item = item.to(self.device).squeeze(dim=0)\n",
    "                label = label.to(self.device).float().squeeze(dim=0)\n",
    "\n",
    "                user_neg, item_neg, label_neg = self._negative_sampling(user, 100)\n",
    "                input_user, input_item, input_label = self._concat(user, user_neg, item, item_neg, label, label_neg)\n",
    "\n",
    "                output = self.model(input_user, input_item)\n",
    "                _, indices = torch.topk(output, dim=0, k=self.topK)\n",
    "                rank_list = torch.take(input_item, indices).cpu().numpy()\n",
    "                target_item = item.cpu().numpy()\n",
    "\n",
    "                epoch_recall += self._get_Recall(rank_list, target_item)\n",
    "                epoch_ndcg += self._get_NDCG(rank_list, target_item)\n",
    "        \n",
    "        avg_recall = epoch_recall / size\n",
    "        avg_ndcg = epoch_ndcg / size\n",
    "\n",
    "        return avg_recall, avg_ndcg\n",
    "\n",
    "    \n",
    "    \n",
    "    def _get_Recall(self, rank_list, target_item_list):        \n",
    "        hit_list = np.intersect1d(target_item_list, rank_list)        \n",
    "        return len(hit_list) / len(target_item_list)\n",
    "\n",
    "    \n",
    "    def _get_DCG(self, rank_list, target_item_list):\n",
    "        DCG = []\n",
    "        for i in range(len(rank_list)):\n",
    "            item = rank_list[i]\n",
    "            if item in target_item_list:\n",
    "                DCG.append(1/np.log(i+2))\n",
    "        \n",
    "        return sum(DCG)\n",
    "        \n",
    "    \n",
    "    def _get_IDCG(self, target_item_list):\n",
    "        IDCG = []\n",
    "        for i in range(len(target_item_list)):\n",
    "            IDCG.append(1/np.log(i+2))\n",
    "        \n",
    "        return sum(IDCG)\n",
    "            \n",
    "\n",
    "    def _get_NDCG(self, rank_list, target_item_list):\n",
    "        DCG = self._get_DCG(rank_list, target_item_list)\n",
    "        IDCG = self._get_IDCG(target_item_list)        \n",
    "        return DCG/IDCG\n",
    "\n",
    "            \n",
    "    def _negative_sampling(self, users, num_neg):\n",
    "        user_neg, item_neg, label_neg = [], [], []\n",
    "\n",
    "        users = users.detach().cpu().numpy()\n",
    "        for user in users:\n",
    "            items = np.random.choice(self.neg_samples[user], min(len(self.neg_samples), num_neg), replace=False)\n",
    "            for item in items:\n",
    "                user_neg.append(user)\n",
    "                item_neg.append(item)\n",
    "                label_neg.append(0)\n",
    "\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(user_neg).to(torch.int64).to(self.device), \n",
    "            torch.tensor(item_neg).to(torch.int64).to(self.device), \n",
    "            torch.tensor(label_neg).to(self.device).float()\n",
    "        )\n",
    "\n",
    "\n",
    "    def _concat(self, user, user_neg, item, item_neg, label, label_neg):\n",
    "        input_user = torch.cat([user, user_neg], dim=0)\n",
    "        input_item = torch.cat([item, item_neg], dim=0)\n",
    "        input_label = torch.cat([label, label_neg], dim=0)\n",
    "\n",
    "        return  input_user, input_item, input_label\n",
    "    \n",
    "\n",
    "    def _multiple_users(self, users, items, num_neg):\n",
    "        user_pos, item_pos, label_pos = [], [], []\n",
    "        users = users.detach().cpu().numpy()\n",
    "        items = items.detach().cpu().numpy()\n",
    "\n",
    "        for user, item in zip(users, items):\n",
    "            user_pos.extend([user] * num_neg)\n",
    "            item_pos.extend([item] * num_neg)\n",
    "            label_pos.extend([1] * num_neg)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(user_pos).to(torch.int64).to(self.device), \n",
    "            torch.tensor(item_pos).to(torch.int64).to(self.device), \n",
    "            torch.tensor(label_pos).to(self.device).float()\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 환경 설정 및 hyper parameter 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_base = os.path.join(os.path.join('/opt','ml','input','data'))\n",
    "dir_data = os.path.join(dir_base, 'train')\n",
    "path_rating = os.path.join(dir_data, 'train_ratings.csv')\n",
    "dir_output = os.path.join(os.getcwd(), 'output')\n",
    "dir_file_path = {\n",
    "    'dir_base': dir_base,\n",
    "    'dir_data': dir_data,\n",
    "    'rating': path_rating,\n",
    "    'dir_output': dir_output,\n",
    "}\n",
    "\n",
    "path = Box(dir_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Use {device}')\n",
    "\n",
    "# 해당 논문의 github에서 사용한 값 참고\n",
    "config = {\n",
    "    'seed': 42,\n",
    "    'device': device,\n",
    "    }\n",
    "\n",
    "config = Box(config)\n",
    "\n",
    "hyper = {\n",
    "    'batch_size': 256,\n",
    "    'epochs': 5,\n",
    "    'pretrain_epochs': 3,\n",
    "    'latent_dim': 16,\n",
    "    'lr': 0.001,\n",
    "    'layers': [32, 16, 8],\n",
    "    'dropout': 0.2,\n",
    "    'topK': 10,\n",
    "    'num_neg': 4,\n",
    "    'num_valid': 1,\n",
    "}\n",
    "\n",
    "hyper = Box(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6807 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "pick trainable feedback...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6807/6807 [00:51<00:00, 131.17it/s]\n",
      "  0%|          | 10/31360 [00:00<05:15, 99.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doen!\n",
      "sorted by sequence and make negative smaples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 6122/31360 [01:01<04:37, 91.01it/s] "
     ]
    }
   ],
   "source": [
    "setSeed(config.seed)\n",
    "data = DataInfo(path.rating, hyper.num_valid)\n",
    "\n",
    "torch.cuda.empty_cache() # if necessary\n",
    "\n",
    "train_dataset = TrainDataset(data.df_pos_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyper.batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "test_dataset = ValidDataset(data.df_pos_test, data.num_users)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "criterion = BPRLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. GMF 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMF_model = GMF(data.num_users, data.num_movies, hyper.latent_dim)\n",
    "GMF_model.to(config.device)\n",
    "GMF_optimizer = torch.optim.Adam(GMF_model.parameters(), lr=hyper.lr)\n",
    "\n",
    "GMF_trainer = Trainer(GMF_model, GMF_optimizer, train_loader, test_loader, hyper.epochs, criterion, config.device, data.user_negative_samples, hyper.num_neg, hyper.topK, path.dir_output, 'GMF_BPR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. MLP 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model = MLP(data.num_users, data.num_movies, hyper.latent_dim, hyper.dropout, hyper.layers)\n",
    "MLP_model.to(config.device)\n",
    "MLP_optimizer = torch.optim.Adam(MLP_model.parameters(), lr=hyper.lr)\n",
    "\n",
    "MLP_trainer = Trainer(MLP_model, MLP_optimizer, train_loader, test_loader, hyper.epochs, criterion, config.device, data.user_negative_samples, hyper.num_neg, hyper.topK, path.dir_output, 'MLP_BPR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. NeuMF 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_GMF_model = GMF(data.num_users, data.num_movies, hyper.latent_dim)\n",
    "best_GMF_model.load_state_dict(torch.load(os.path.join(path.dir_output, 'best_NDCG_GMF_BPR.pt')))\n",
    "\n",
    "best_MLP_model = MLP(data.num_users, data.num_movies, hyper.latent_dim, hyper.dropout, hyper.layers)\n",
    "best_MLP_model.load_state_dict(torch.load(os.path.join(path.dir_output, 'best_NDCG_MLP_BPR.pt')))\n",
    "\n",
    "NCF_model = NCF(best_GMF_model, best_MLP_model, hyper.latent_dim, hyper.layers)\n",
    "NCF_model.to(config.device)\n",
    "NCF_optimizer = torch.optim.Adam(NeuMF_model.parameters(), lr=hyper.lr) # torch.optim.SGD(NCF_model.parameters(), lr=hyper.lr)\n",
    "\n",
    "NCF_trainer = Trainer(NCF_model, NCF_optimizer, train_loader, test_loader, hyper.pretrain_epochs, criterion, config.device, data.user_negative_samples, hyper.num_neg, hyper.topK, path.dir_output, 'NCF_BPR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Plot loss, hr, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(epochs, all_loss, model_name, dir_output, loss_name=['Loss', 'Recall', 'NDCG']):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(30, 7))\n",
    "    fig.suptitle(model_name, fontsize=30)\n",
    "    x_list = [i for i in range(1, epochs+1)]\n",
    "    \n",
    "    for i in range(3):\n",
    "        sns.lineplot(\n",
    "            x=x_list, y=all_loss[i],\n",
    "            ax = axes[i]\n",
    "        )\n",
    "\n",
    "        axes[i].set_ylabel(loss_name[i])\n",
    "        axes[i].set_xlabel('Epochs')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(dir_output, f'{model_name}.png'), dpi=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss = [GMF_trainer.loss_list, GMF_trainer.recall_list, GMF_trainer.ndcg_list]\n",
    "plot_loss(hyper.epochs, all_loss, 'GMF', path.dir_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss = [MLP_trainer.loss_list, MLP_trainer.recall_list, MLP_trainer.ndcg_list]\n",
    "plot_loss(hyper.epochs, all_loss, 'MLP', path.dir_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss = [NCF_trainer.loss_list, NCF_trainer.recall_list, NCF_trainer.ndcg_list]\n",
    "plot_loss(hyper.pretrain_epochs, all_loss, 'NCF', path.dir_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model load\n",
    "\n",
    "best_model = NCF(GMF_model, MLP_model, hyper.latent_dim, hyper.layers)\n",
    "best_model.load_state_dict(torch.load(os.path.join(path.dir_output, 'best_NDCG_NCF_BPR.pt')))\n",
    "\n",
    "best_model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, users, negative_samples, topK, device, user_encoder, item_encoder, dir_output):\n",
    "        model.eval()\n",
    "\n",
    "        # columns: user_id\n",
    "        df_item_user = pd.DataFrame(columns=user_encoder.inverse_transform([i for i in range(users)]))\n",
    "        df_item_user.columns.name = 'user'\n",
    "        df_item_user.index.name = 'order'\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for user in tqdm(range(users)):\n",
    "                neg_items = negative_samples[user]\n",
    "\n",
    "                users, items = user_item_to_tensor(user, neg_items, device)\n",
    "\n",
    "                output = model(users, items)\n",
    "                _, indices = torch.topk(output, dim=0, k=topK)\n",
    "                rank_list = torch.take(items, indices).cpu().numpy().tolist()\n",
    "\n",
    "                origin_user = user_encoder.inverse_transform([user])[0]\n",
    "                origin_items = item_encoder.inverse_transform(rank_list)\n",
    "            \n",
    "                df_item_user[origin_user] = origin_items\n",
    "        \n",
    "        df_prediction = pd.DataFrame(df_item_user.unstack()).reset_index()[['user', 0]]\n",
    "        df_prediction.rename(columns={0: 'item'}, inplace=True)\n",
    "        df_prediction.to_csv(os.path.join(dir_output, 'submission_BPR.csv'), index=False)\n",
    "        return df_prediction\n",
    "\n",
    "\n",
    "def user_item_to_tensor(user, negative_items, device):        \n",
    "    users = [user] * len(negative_items)\n",
    "    items = negative_items\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(users).to(torch.int64).to(device), \n",
    "        torch.tensor(items).to(torch.int64).to(device),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(best_model, data.num_users, data.user_negative_samples, hyper.topK, config.device, data.user_encoder, data.movie_encoder, path.dir_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
